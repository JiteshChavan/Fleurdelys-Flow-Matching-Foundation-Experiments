{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798ea072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FinalLayer (nn.Module):\n",
    "    \"\"\"\n",
    "    Final layer of the backbone\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, patch_size, out_channels):\n",
    "        super().__init__()\n",
    "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
    "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n",
    "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size, bias=True))\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        scale, shift = self.adaLN_modulation(y).chunk(2, dim=-1) # 2x (B, C)\n",
    "        x = modulate(self.norm_final(x), shift, scale) # x = x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1) -> (B, T, C)\n",
    "        x = self.linear(x) # (B, T, C) - > (B, T, patch_size * patch_size * out_channels)\n",
    "        return x\n",
    "\n",
    "def modulate(x, shift, scale):\n",
    "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9724c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = FinalLayer(7, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b179bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(torch.randn(1, 3, 7), torch.randn(1, 7)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2040df0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0447, -0.0520, -0.0628,  0.8552])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class activation(nn.Module):\n",
    "    def __init__(self, activation_layer= nn.GELU):\n",
    "        super().__init__()\n",
    "        self.act = activation_layer\n",
    "    \n",
    "    def forward (self, x):\n",
    "        return self.act(x)\n",
    "\n",
    "x = torch.randn(4)\n",
    "a = activation(activation_layer=nn.GELU(approximate=\"tanh\"))\n",
    "a(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ab2cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7365e-05],\n",
       "        [-1.8660e-03]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class GatedMLP(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            fan_in: int,\n",
    "            fan_h: int = None,\n",
    "            fan_out: int = None,\n",
    "            act_layer = lambda:nn.GELU(approximate=\"tanh\"),\n",
    "            drop: float = 0.0,\n",
    "            bias: bool = True,\n",
    "    )-> None:\n",
    "        super().__init__()\n",
    "        fan_out = fan_out or fan_in # stores first truth value\n",
    "        fan_h = fan_h or fan_in\n",
    "        self.fc1 = nn.Linear(fan_in, 2*fan_h, bias=bias)\n",
    "        self.fc2 = nn.Linear(fan_h, fan_out, bias=bias)\n",
    "        self.act_layer = act_layer()\n",
    "    \n",
    "    def forward(self, x:Tensor)-> Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x, scale = x.chunk(2, dim=-1)\n",
    "        x = self.act_layer(x) * scale\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# works! : when init calls approx_gelu(), it instantiates the GELU object\n",
    "approx_gelu = lambda: nn.GELU(approximate=\"tanh\")\n",
    "m = GatedMLP(1, 2, act_layer=approx_gelu, drop=0, bias=False)\n",
    "x = torch.randn(2, 1)\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e4c7884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function __main__.<lambda>()>, torch.nn.modules.activation.GELU)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_gelu, nn.GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6ac6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
